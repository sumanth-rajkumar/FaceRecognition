{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212c7931-4cf6-429d-9d0a-510b5eed1b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename 24_0_1_20170116220224657 .jpg.chip.jpg did not match the pattern.\n",
      "Filename 39_1_20170116174525125.jpg.chip.jpg did not match the pattern.\n",
      "Filename 55_0_0_20170116232725357jpg.chip.jpg did not match the pattern.\n",
      "Filename 61_1_20170109142408075.jpg.chip.jpg did not match the pattern.\n",
      "Filename 61_1_20170109150557335.jpg.chip.jpg did not match the pattern.\n",
      "                                 img_name  age  gender  ethnicity\n",
      "0  100_0_0_20170112213500903.jpg.chip.jpg  100       0          0\n",
      "1  100_0_0_20170112215240346.jpg.chip.jpg  100       0          0\n",
      "2  100_1_0_20170110183726390.jpg.chip.jpg  100       1          0\n",
      "3  100_1_0_20170112213001988.jpg.chip.jpg  100       1          0\n",
      "4  100_1_0_20170112213303693.jpg.chip.jpg  100       1          0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23703 entries, 0 to 23702\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   img_name   23703 non-null  object\n",
      " 1   age        23703 non-null  int64 \n",
      " 2   gender     23703 non-null  int64 \n",
      " 3   ethnicity  23703 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 740.8+ KB\n",
      "None\n",
      "gender_labels_cat shape: (23703, 2)\n",
      "ethnicity_labels_cat shape: (23703, 5)\n",
      "Number of training samples: 16591\n",
      "Number of validation samples: 3556\n",
      "Number of test samples: 3556\n",
      "Model output names: ListWrapper(['age_output', 'ethnicity_output', 'gender_output'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suman\\face_recognition\\env\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 331ms/step - age_output_loss: 440.5686 - age_output_mae: 15.8793 - ethnicity_output_accuracy: 0.2774 - ethnicity_output_loss: 3.3347 - gender_output_accuracy: 0.5361 - gender_output_loss: 1.6596 - loss: 445.5641 - val_age_output_loss: 199.2537 - val_age_output_mae: 11.0607 - val_ethnicity_output_accuracy: 0.4272 - val_ethnicity_output_loss: 1.4254 - val_gender_output_accuracy: 0.6746 - val_gender_output_loss: 0.6126 - val_loss: 200.5428\n",
      "Epoch 2/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 226ms/step - age_output_loss: 208.3466 - age_output_mae: 10.9048 - ethnicity_output_accuracy: 0.4028 - ethnicity_output_loss: 1.4812 - gender_output_accuracy: 0.6337 - gender_output_loss: 0.6627 - loss: 210.6163 - val_age_output_loss: 136.5079 - val_age_output_mae: 8.8649 - val_ethnicity_output_accuracy: 0.4663 - val_ethnicity_output_loss: 1.3184 - val_gender_output_accuracy: 0.6884 - val_gender_output_loss: 0.5891 - val_loss: 139.3689\n",
      "Epoch 3/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 221ms/step - age_output_loss: 157.6497 - age_output_mae: 9.4702 - ethnicity_output_accuracy: 0.4425 - ethnicity_output_loss: 1.3829 - gender_output_accuracy: 0.6402 - gender_output_loss: 0.6476 - loss: 159.6070 - val_age_output_loss: 127.5933 - val_age_output_mae: 8.5725 - val_ethnicity_output_accuracy: 0.4769 - val_ethnicity_output_loss: 1.2925 - val_gender_output_accuracy: 0.7109 - val_gender_output_loss: 0.5840 - val_loss: 129.5182\n",
      "Epoch 4/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 219ms/step - age_output_loss: 134.3034 - age_output_mae: 8.7498 - ethnicity_output_accuracy: 0.4553 - ethnicity_output_loss: 1.3601 - gender_output_accuracy: 0.6409 - gender_output_loss: 0.6442 - loss: 136.2854 - val_age_output_loss: 119.0594 - val_age_output_mae: 7.9669 - val_ethnicity_output_accuracy: 0.4859 - val_ethnicity_output_loss: 1.2931 - val_gender_output_accuracy: 0.6912 - val_gender_output_loss: 0.5942 - val_loss: 120.5336\n",
      "Epoch 5/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 225ms/step - age_output_loss: 117.9942 - age_output_mae: 8.2088 - ethnicity_output_accuracy: 0.4592 - ethnicity_output_loss: 1.3530 - gender_output_accuracy: 0.6515 - gender_output_loss: 0.6360 - loss: 120.0001 - val_age_output_loss: 107.1691 - val_age_output_mae: 7.7096 - val_ethnicity_output_accuracy: 0.5039 - val_ethnicity_output_loss: 1.2711 - val_gender_output_accuracy: 0.7345 - val_gender_output_loss: 0.5492 - val_loss: 109.7100\n",
      "Epoch 6/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 226ms/step - age_output_loss: 107.7059 - age_output_mae: 7.7573 - ethnicity_output_accuracy: 0.4594 - ethnicity_output_loss: 1.3502 - gender_output_accuracy: 0.6775 - gender_output_loss: 0.6130 - loss: 109.6987 - val_age_output_loss: 109.1501 - val_age_output_mae: 7.9561 - val_ethnicity_output_accuracy: 0.5076 - val_ethnicity_output_loss: 1.2972 - val_gender_output_accuracy: 0.7539 - val_gender_output_loss: 0.5405 - val_loss: 111.2080\n",
      "Epoch 7/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 223ms/step - age_output_loss: 101.9263 - age_output_mae: 7.5852 - ethnicity_output_accuracy: 0.4588 - ethnicity_output_loss: 1.3495 - gender_output_accuracy: 0.6573 - gender_output_loss: 0.6361 - loss: 103.9359 - val_age_output_loss: 92.4871 - val_age_output_mae: 7.1813 - val_ethnicity_output_accuracy: 0.5177 - val_ethnicity_output_loss: 1.2897 - val_gender_output_accuracy: 0.7461 - val_gender_output_loss: 0.5476 - val_loss: 94.2163\n",
      "Epoch 8/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 220ms/step - age_output_loss: 97.8768 - age_output_mae: 7.4165 - ethnicity_output_accuracy: 0.4585 - ethnicity_output_loss: 1.3398 - gender_output_accuracy: 0.6836 - gender_output_loss: 0.6043 - loss: 99.8112 - val_age_output_loss: 97.3465 - val_age_output_mae: 7.3031 - val_ethnicity_output_accuracy: 0.4685 - val_ethnicity_output_loss: 1.2931 - val_gender_output_accuracy: 0.7508 - val_gender_output_loss: 0.5262 - val_loss: 99.5716\n",
      "Epoch 9/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 229ms/step - age_output_loss: 88.6045 - age_output_mae: 7.0648 - ethnicity_output_accuracy: 0.4593 - ethnicity_output_loss: 1.3512 - gender_output_accuracy: 0.6896 - gender_output_loss: 0.5980 - loss: 90.5575 - val_age_output_loss: 87.5308 - val_age_output_mae: 6.9073 - val_ethnicity_output_accuracy: 0.4688 - val_ethnicity_output_loss: 1.2980 - val_gender_output_accuracy: 0.7821 - val_gender_output_loss: 0.4886 - val_loss: 89.2981\n",
      "Epoch 10/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 224ms/step - age_output_loss: 86.8259 - age_output_mae: 6.9471 - ethnicity_output_accuracy: 0.4564 - ethnicity_output_loss: 1.3472 - gender_output_accuracy: 0.7059 - gender_output_loss: 0.5763 - loss: 88.7499 - val_age_output_loss: 87.5575 - val_age_output_mae: 6.9302 - val_ethnicity_output_accuracy: 0.4761 - val_ethnicity_output_loss: 1.2667 - val_gender_output_accuracy: 0.7573 - val_gender_output_loss: 0.5126 - val_loss: 89.7975\n",
      "Epoch 11/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 222ms/step - age_output_loss: 79.1874 - age_output_mae: 6.6631 - ethnicity_output_accuracy: 0.4572 - ethnicity_output_loss: 1.3481 - gender_output_accuracy: 0.6947 - gender_output_loss: 0.5909 - loss: 81.1517 - val_age_output_loss: 88.4334 - val_age_output_mae: 7.0932 - val_ethnicity_output_accuracy: 0.5405 - val_ethnicity_output_loss: 1.2351 - val_gender_output_accuracy: 0.7725 - val_gender_output_loss: 0.5080 - val_loss: 89.9208\n",
      "Epoch 12/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 223ms/step - age_output_loss: 73.5873 - age_output_mae: 6.4406 - ethnicity_output_accuracy: 0.4691 - ethnicity_output_loss: 1.3265 - gender_output_accuracy: 0.6994 - gender_output_loss: 0.5798 - loss: 75.4933 - val_age_output_loss: 84.5052 - val_age_output_mae: 6.7538 - val_ethnicity_output_accuracy: 0.4620 - val_ethnicity_output_loss: 1.2977 - val_gender_output_accuracy: 0.7843 - val_gender_output_loss: 0.4824 - val_loss: 86.3786\n",
      "Epoch 13/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 224ms/step - age_output_loss: 70.9484 - age_output_mae: 6.3059 - ethnicity_output_accuracy: 0.4634 - ethnicity_output_loss: 1.3426 - gender_output_accuracy: 0.7175 - gender_output_loss: 0.5645 - loss: 72.8480 - val_age_output_loss: 99.1991 - val_age_output_mae: 7.2505 - val_ethnicity_output_accuracy: 0.4440 - val_ethnicity_output_loss: 1.2822 - val_gender_output_accuracy: 0.7103 - val_gender_output_loss: 0.5390 - val_loss: 100.9222\n",
      "Epoch 14/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 222ms/step - age_output_loss: 67.1227 - age_output_mae: 6.1714 - ethnicity_output_accuracy: 0.4648 - ethnicity_output_loss: 1.3416 - gender_output_accuracy: 0.7152 - gender_output_loss: 0.5733 - loss: 69.0355 - val_age_output_loss: 97.2722 - val_age_output_mae: 7.2171 - val_ethnicity_output_accuracy: 0.5501 - val_ethnicity_output_loss: 1.2129 - val_gender_output_accuracy: 0.6935 - val_gender_output_loss: 0.5640 - val_loss: 99.7158\n",
      "Epoch 15/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 224ms/step - age_output_loss: 62.6353 - age_output_mae: 5.9955 - ethnicity_output_accuracy: 0.4702 - ethnicity_output_loss: 1.3211 - gender_output_accuracy: 0.7259 - gender_output_loss: 0.5626 - loss: 64.5208 - val_age_output_loss: 85.7333 - val_age_output_mae: 6.7240 - val_ethnicity_output_accuracy: 0.5222 - val_ethnicity_output_loss: 1.2159 - val_gender_output_accuracy: 0.7697 - val_gender_output_loss: 0.4946 - val_loss: 86.2210\n",
      "Epoch 16/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 222ms/step - age_output_loss: 58.7347 - age_output_mae: 5.7635 - ethnicity_output_accuracy: 0.4738 - ethnicity_output_loss: 1.3089 - gender_output_accuracy: 0.7194 - gender_output_loss: 0.5652 - loss: 60.5710 - val_age_output_loss: 87.4523 - val_age_output_mae: 6.8371 - val_ethnicity_output_accuracy: 0.5121 - val_ethnicity_output_loss: 1.2192 - val_gender_output_accuracy: 0.7750 - val_gender_output_loss: 0.4785 - val_loss: 88.8544\n",
      "Epoch 17/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 222ms/step - age_output_loss: 58.4609 - age_output_mae: 5.7142 - ethnicity_output_accuracy: 0.4776 - ethnicity_output_loss: 1.3010 - gender_output_accuracy: 0.7275 - gender_output_loss: 0.5575 - loss: 60.3195 - val_age_output_loss: 88.4968 - val_age_output_mae: 6.8999 - val_ethnicity_output_accuracy: 0.5278 - val_ethnicity_output_loss: 1.2122 - val_gender_output_accuracy: 0.7840 - val_gender_output_loss: 0.4684 - val_loss: 90.7625\n",
      "Epoch 18/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 222ms/step - age_output_loss: 57.5105 - age_output_mae: 5.7005 - ethnicity_output_accuracy: 0.4693 - ethnicity_output_loss: 1.3280 - gender_output_accuracy: 0.7181 - gender_output_loss: 0.5640 - loss: 59.2160 - val_age_output_loss: 87.1138 - val_age_output_mae: 6.8968 - val_ethnicity_output_accuracy: 0.5408 - val_ethnicity_output_loss: 1.2122 - val_gender_output_accuracy: 0.7821 - val_gender_output_loss: 0.4622 - val_loss: 89.2770\n",
      "Epoch 19/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 222ms/step - age_output_loss: 54.3117 - age_output_mae: 5.4977 - ethnicity_output_accuracy: 0.4701 - ethnicity_output_loss: 1.3100 - gender_output_accuracy: 0.7178 - gender_output_loss: 0.5601 - loss: 56.1622 - val_age_output_loss: 90.3328 - val_age_output_mae: 6.8326 - val_ethnicity_output_accuracy: 0.5121 - val_ethnicity_output_loss: 1.2253 - val_gender_output_accuracy: 0.7857 - val_gender_output_loss: 0.4734 - val_loss: 90.5847\n",
      "Epoch 20/50\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 221ms/step - age_output_loss: 53.7036 - age_output_mae: 5.5104 - ethnicity_output_accuracy: 0.4726 - ethnicity_output_loss: 1.3139 - gender_output_accuracy: 0.7249 - gender_output_loss: 0.5512 - loss: 55.5006 - val_age_output_loss: 88.0715 - val_age_output_mae: 6.8422 - val_ethnicity_output_accuracy: 0.4899 - val_ethnicity_output_loss: 1.2463 - val_gender_output_accuracy: 0.7770 - val_gender_output_loss: 0.4730 - val_loss: 90.1375\n",
      "112/112 - 33s - 294ms/step - age_output_loss: 82.6847 - age_output_mae: 6.6521 - ethnicity_output_accuracy: 0.5186 - ethnicity_output_loss: 1.2249 - gender_output_accuracy: 0.7629 - gender_output_loss: 0.4943 - loss: 84.9217\n",
      "Test loss and metrics: [84.9216537475586, 82.68470001220703, 1.2249336242675781, 0.49434903264045715, 6.652089595794678, 0.5185601711273193, 0.7629358768463135]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define image dimensions\n",
    "IMG_HEIGHT = 128  # You can reduce this to 64 if you still face memory issues\n",
    "IMG_WIDTH = 128\n",
    "batch_size = 32  # Adjust as needed based on your system's memory\n",
    "\n",
    "# Define the dataset directory\n",
    "dataset_dir = 'UTKFace'  # Replace with your actual dataset directory\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(dataset_dir) if f.endswith('.jpg')]\n",
    "\n",
    "# Extract labels from filenames and create DataFrame\n",
    "data = []\n",
    "\n",
    "# Regular expression pattern to extract labels\n",
    "pattern = r'^(\\d+)_(\\d+)_(\\d+)_\\d+\\.jpg\\.chip\\.jpg$'\n",
    "# Or use the optional .chip part\n",
    "# pattern = r'^(\\d+)_(\\d+)_(\\d+)_\\d+\\.jpg(?:\\.chip)?\\.jpg$'\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "for img_name in image_files:\n",
    "    match = re.match(pattern, img_name)\n",
    "    if match:\n",
    "        age = int(match.group(1))\n",
    "        gender = int(match.group(2))\n",
    "        ethnicity = int(match.group(3))\n",
    "        data.append({'img_name': img_name, 'age': age, 'gender': gender, 'ethnicity': ethnicity})\n",
    "    else:\n",
    "        print(f'Filename {img_name} did not match the pattern.')\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Verify the DataFrame\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Extract labels\n",
    "age_labels = df['age'].values\n",
    "gender_labels = df['gender'].values\n",
    "ethnicity_labels = df['ethnicity'].values\n",
    "\n",
    "# One-Hot Encode Gender and Ethnicity Labels\n",
    "gender_labels_cat = to_categorical(gender_labels, num_classes=2)\n",
    "ethnicity_labels_cat = to_categorical(ethnicity_labels, num_classes=5)\n",
    "\n",
    "# Verify the shapes of the one-hot encoded labels\n",
    "print('gender_labels_cat shape:', gender_labels_cat.shape)\n",
    "print('ethnicity_labels_cat shape:', ethnicity_labels_cat.shape)\n",
    "\n",
    "# Split the data into training+validation and test sets\n",
    "\n",
    "# First, split into training+validation and test sets\n",
    "filenames = df['img_name'].values\n",
    "\n",
    "filenames_temp, filenames_test, age_temp, age_test, gender_temp, gender_test, ethnicity_temp, ethnicity_test = train_test_split(\n",
    "    filenames, age_labels, gender_labels_cat, ethnicity_labels_cat, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "filenames_train, filenames_val, age_train, age_val, ethnicity_train, ethnicity_val, gender_train, gender_val = train_test_split(\n",
    "    filenames_temp, age_temp, ethnicity_temp, gender_temp, test_size=0.1765, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Verify the shapes after splitting\n",
    "print('Number of training samples:', len(filenames_train))\n",
    "print('Number of validation samples:', len(filenames_val))\n",
    "print('Number of test samples:', len(filenames_test))\n",
    "\n",
    "# Prepare labels as dictionaries\n",
    "train_labels = {\n",
    "    'age_output': age_train,\n",
    "    'gender_output': gender_train,\n",
    "    'ethnicity_output': ethnicity_train\n",
    "}\n",
    "\n",
    "val_labels = {\n",
    "    'age_output': age_val,\n",
    "    'gender_output': gender_val,\n",
    "    'ethnicity_output': ethnicity_val\n",
    "}\n",
    "\n",
    "test_labels = {\n",
    "    'age_output': age_test,\n",
    "    'gender_output': gender_test,\n",
    "    'ethnicity_output': ethnicity_test\n",
    "}\n",
    "\n",
    "# Create custom data generator\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_filenames, labels, batch_size, img_dir, img_height, img_width, shuffle=True):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_dir = img_dir\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.image_filenames))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_filenames) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate indexes of the batch\n",
    "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        batch_filenames = [self.image_filenames[k] for k in batch_indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(batch_filenames, batch_indexes)\n",
    "        \n",
    "        return X, y  # Return labels as a dictionary\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, batch_filenames, batch_indexes):\n",
    "        X = []\n",
    "        # Initialize label dictionaries for this batch\n",
    "        y = {\n",
    "            'age_output': self.labels['age_output'][batch_indexes],\n",
    "            'gender_output': self.labels['gender_output'][batch_indexes],\n",
    "            'ethnicity_output': self.labels['ethnicity_output'][batch_indexes]\n",
    "        }\n",
    "    \n",
    "        \n",
    "    \n",
    "        # Rest of your code...\n",
    "\n",
    "        \n",
    "        for filename in batch_filenames:\n",
    "            img_path = os.path.join(self.img_dir, filename)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img = img.convert('RGB')\n",
    "                img = img.resize((self.img_width, self.img_height))\n",
    "                img_array = img_to_array(img)\n",
    "                img_array = img_array / 255.0\n",
    "                X.append(img_array)\n",
    "            except Exception as e:\n",
    "                print(f'Error processing image {img_path}: {e}')\n",
    "                # Append a zero array in case of error to keep batch size consistent\n",
    "                X.append(np.zeros((self.img_height, self.img_width, 3)))\n",
    "        \n",
    "        X = np.array(X)\n",
    "        return X, y\n",
    "\n",
    "# Create generators\n",
    "train_generator = DataGenerator(\n",
    "    filenames_train,\n",
    "    train_labels,\n",
    "    batch_size=batch_size,\n",
    "    img_dir=dataset_dir,\n",
    "    img_height=IMG_HEIGHT,\n",
    "    img_width=IMG_WIDTH\n",
    ")\n",
    "\n",
    "val_generator = DataGenerator(\n",
    "    filenames_val,\n",
    "    val_labels,\n",
    "    batch_size=batch_size,\n",
    "    img_dir=dataset_dir,\n",
    "    img_height=IMG_HEIGHT,\n",
    "    img_width=IMG_WIDTH\n",
    ")\n",
    "\n",
    "test_generator = DataGenerator(\n",
    "    filenames_test,\n",
    "    test_labels,\n",
    "    batch_size=batch_size,\n",
    "    img_dir=dataset_dir,\n",
    "    img_height=IMG_HEIGHT,\n",
    "    img_width=IMG_WIDTH,\n",
    "    shuffle=False  # No need to shuffle test data\n",
    ")\n",
    "\n",
    "# Build the multi-task learning model\n",
    "input_layer = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# Shared Convolutional Layers\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output Layers with Correct Names\n",
    "# Use tf.identity to assign names to the output tensors\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "# Output Layers with Correct Names using Lambda\n",
    "age_output = Lambda(lambda z: z, name='age_output')(Dense(1)(x))\n",
    "gender_output = Lambda(lambda z: z, name='gender_output')(Dense(2, activation='softmax')(x))\n",
    "ethnicity_output = Lambda(lambda z: z, name='ethnicity_output')(Dense(5, activation='softmax')(x))\n",
    "\n",
    "\n",
    "outputs = {\n",
    "    'age_output': age_output,\n",
    "    'gender_output': gender_output,\n",
    "    'ethnicity_output': ethnicity_output\n",
    "}\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "\n",
    "\n",
    "# Verify the model's output names\n",
    "print('Model output names:', model.output_names)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'age_output': 'mse',\n",
    "        'gender_output': 'categorical_crossentropy',\n",
    "        'ethnicity_output': 'categorical_crossentropy'\n",
    "    },\n",
    "    metrics={\n",
    "        'age_output': 'mae',\n",
    "        'gender_output': 'accuracy',\n",
    "        'ethnicity_output': 'accuracy'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set up callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_multitask_model.keras',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss'\n",
    ")\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# Train the model using generators\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate(test_generator, verbose=2)\n",
    "\n",
    "# Print the evaluation results\n",
    "print('Test loss and metrics:', test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da08f46-e063-4bc6-a84d-3da8fb03c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Distribution:\n",
      "gender\n",
      "0    12389\n",
      "1    11314\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "gender_counts = df['gender'].value_counts()\n",
    "print(\"Gender Distribution:\")\n",
    "print(gender_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509c4fd0-0ea9-4577-9f46-6011a51d601a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ethnicity Distribution:\n",
      "ethnicity\n",
      "0    10077\n",
      "1     4525\n",
      "3     3975\n",
      "2     3434\n",
      "4     1692\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ethnicity_counts = df['ethnicity'].value_counts()\n",
    "print(\"\\nEthnicity Distribution:\")\n",
    "print(ethnicity_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d89fc8d-9b7a-4a97-9cb0-4d13fcadb17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     y_true_gender\u001b[38;5;241m.\u001b[39mextend(np\u001b[38;5;241m.\u001b[39margmax(y_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender_output\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     10\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_batch)\n\u001b[1;32m---> 11\u001b[0m     y_pred_gender\u001b[38;5;241m.\u001b[39mextend(np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Assuming gender_output is the second output\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_true_gender, y_pred_gender, target_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFemale\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get true and predicted labels for gender\n",
    "y_true_gender = []\n",
    "y_pred_gender = []\n",
    "\n",
    "for i in range(len(test_generator)):\n",
    "    X_batch, y_batch = test_generator[i]\n",
    "    y_true_gender.extend(np.argmax(y_batch['gender_output'], axis=1))\n",
    "    predictions = model.predict(X_batch)\n",
    "    y_pred_gender.extend(np.argmax(predictions[1], axis=1))  # Assuming gender_output is the second output\n",
    "\n",
    "print(classification_report(y_true_gender, y_pred_gender, target_names=['Male', 'Female']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f2be49f-d4da-4079-8a49-48e92af04257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename 24_0_1_20170116220224657 .jpg.chip.jpg did not match the pattern.\n",
      "Filename 39_1_20170116174525125.jpg.chip.jpg did not match the pattern.\n",
      "Filename 55_0_0_20170116232725357jpg.chip.jpg did not match the pattern.\n",
      "Filename 61_1_20170109142408075.jpg.chip.jpg did not match the pattern.\n",
      "Filename 61_1_20170109150557335.jpg.chip.jpg did not match the pattern.\n",
      "                                 img_name  age  gender  ethnicity\n",
      "0  100_0_0_20170112213500903.jpg.chip.jpg  100       0          0\n",
      "1  100_0_0_20170112215240346.jpg.chip.jpg  100       0          0\n",
      "2  100_1_0_20170110183726390.jpg.chip.jpg  100       1          0\n",
      "3  100_1_0_20170112213001988.jpg.chip.jpg  100       1          0\n",
      "4  100_1_0_20170112213303693.jpg.chip.jpg  100       1          0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23703 entries, 0 to 23702\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   img_name   23703 non-null  object\n",
      " 1   age        23703 non-null  int64 \n",
      " 2   gender     23703 non-null  int64 \n",
      " 3   ethnicity  23703 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 740.8+ KB\n",
      "None\n",
      "\n",
      "Ethnicity Distribution Before Balancing:\n",
      "ethnicity\n",
      "0    10077\n",
      "1     4525\n",
      "3     3975\n",
      "2     3434\n",
      "4     1692\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ethnicity Distribution After Balancing:\n",
      "ethnicity\n",
      "0    10077\n",
      "4    10077\n",
      "3    10077\n",
      "2    10077\n",
      "1    10077\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of training samples: 35268\n",
      "Number of validation samples: 7559\n",
      "Number of test samples: 7558\n",
      "\n",
      "Model output names: ListWrapper(['age_output', 'ethnicity_output', 'gender_output'])\n",
      "\n",
      "Ethnicity class weights: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n",
      "Epoch 1/50\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 293ms/step - age_output_loss: 349.3036 - age_output_mae: 14.0701 - ethnicity_output_accuracy: 0.2264 - ethnicity_output_loss: 2.9893 - gender_output_accuracy: 0.5409 - gender_output_loss: 1.2067 - loss: 352.9739 - val_age_output_loss: 175.5406 - val_age_output_mae: 9.9984 - val_ethnicity_output_accuracy: 0.3322 - val_ethnicity_output_loss: 1.5453 - val_gender_output_accuracy: 0.6575 - val_gender_output_loss: 0.6325 - val_loss: 178.0845\n",
      "Epoch 2/50\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 294ms/step - age_output_loss: 200.9725 - age_output_mae: 10.7084 - ethnicity_output_accuracy: 0.3099 - ethnicity_output_loss: 1.5852 - gender_output_accuracy: 0.6491 - gender_output_loss: 0.6409 - loss: 203.2194 - val_age_output_loss: 144.8951 - val_age_output_mae: 9.2055 - val_ethnicity_output_accuracy: 0.3707 - val_ethnicity_output_loss: 1.4930 - val_gender_output_accuracy: 0.7136 - val_gender_output_loss: 0.6009 - val_loss: 147.0549\n",
      "Epoch 3/50\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 294ms/step - age_output_loss: 160.7077 - age_output_mae: 9.4373 - ethnicity_output_accuracy: 0.3137 - ethnicity_output_loss: 1.5639 - gender_output_accuracy: 0.6465 - gender_output_loss: 0.6457 - loss: 163.0655 - val_age_output_loss: 109.3560 - val_age_output_mae: 7.5897 - val_ethnicity_output_accuracy: 0.3393 - val_ethnicity_output_loss: 1.4959 - val_gender_output_accuracy: 0.6490 - val_gender_output_loss: 0.6225 - val_loss: 111.4023\n",
      "Epoch 4/50\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 298ms/step - age_output_loss: 135.6544 - age_output_mae: 8.6650 - ethnicity_output_accuracy: 0.3026 - ethnicity_output_loss: 1.5613 - gender_output_accuracy: 0.6458 - gender_output_loss: 0.6431 - loss: 137.8643 - val_age_output_loss: 99.8376 - val_age_output_mae: 7.3551 - val_ethnicity_output_accuracy: 0.3041 - val_ethnicity_output_loss: 1.5183 - val_gender_output_accuracy: 0.7012 - val_gender_output_loss: 0.6035 - val_loss: 102.1051\n",
      "Epoch 5/50\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 298ms/step - age_output_loss: 125.6378 - age_output_mae: 8.2916 - ethnicity_output_accuracy: 0.3011 - ethnicity_output_loss: 1.5678 - gender_output_accuracy: 0.6302 - gender_output_loss: 0.6524 - loss: 127.7741 - val_age_output_loss: 93.4141 - val_age_output_mae: 6.9179 - val_ethnicity_output_accuracy: 0.2934 - val_ethnicity_output_loss: 1.5380 - val_gender_output_accuracy: 0.6719 - val_gender_output_loss: 0.6320 - val_loss: 95.4639\n",
      "Epoch 6/50\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - age_output_loss: 118.8009 - age_output_mae: 8.0635 - ethnicity_output_accuracy: 0.2952 - ethnicity_output_loss: 1.5687 - gender_output_accuracy: 0.6264 - gender_output_loss: 0.6646 - loss: 120.9854"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 373\u001b[0m\n\u001b[0;32m    370\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# Train the model using generators\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Removed class_weight parameter\u001b[39;49;00m\n\u001b[0;32m    379\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m    383\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_generator, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\face_recognition\\env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\face_recognition\\env\\Lib\\site-packages\\h5py\\_hl\\files.py:580\u001b[0m, in \u001b[0;36mFile.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# We have to explicitly murder all open objects related to the file\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \n\u001b[0;32m    577\u001b[0m     \u001b[38;5;66;03m# Close file-resident objects first, then the files.\u001b[39;00m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;66;03m# Otherwise we get errors in MPI mode.\u001b[39;00m\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39m_close_open_objects(h5f\u001b[38;5;241m.\u001b[39mOBJ_LOCAL \u001b[38;5;241m|\u001b[39m \u001b[38;5;241m~\u001b[39mh5f\u001b[38;5;241m.\u001b[39mOBJ_FILE)\n\u001b[1;32m--> 580\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_close_open_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOBJ_LOCAL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOBJ_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    583\u001b[0m     _objects\u001b[38;5;241m.\u001b[39mnonlocal_close()\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:355\u001b[0m, in \u001b[0;36mh5py.h5f.FileID._close_open_objects\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5fd.pyx:179\u001b[0m, in \u001b[0;36mh5py.h5fd.H5FD_fileobj_truncate\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample, class_weight\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define image dimensions\n",
    "IMG_HEIGHT = 128  # Adjust as needed\n",
    "IMG_WIDTH = 128\n",
    "batch_size = 32  # Adjust as needed based on your system's memory\n",
    "\n",
    "# Define the dataset directory\n",
    "dataset_dir = 'UTKFace'  # Replace with your actual dataset directory\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(dataset_dir) if f.endswith('.jpg')]\n",
    "\n",
    "# Extract labels from filenames and create DataFrame\n",
    "data = []\n",
    "\n",
    "# Regular expression pattern to extract labels\n",
    "pattern = r'^(\\d+)_(\\d+)_(\\d+)_\\d+\\.jpg(?:\\.chip)?\\.jpg$'\n",
    "\n",
    "for img_name in image_files:\n",
    "    match = re.match(pattern, img_name)\n",
    "    if match:\n",
    "        age = int(match.group(1))\n",
    "        gender = int(match.group(2))\n",
    "        ethnicity = int(match.group(3))\n",
    "        data.append({'img_name': img_name, 'age': age, 'gender': gender, 'ethnicity': ethnicity})\n",
    "    else:\n",
    "        print(f'Filename {img_name} did not match the pattern.')\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Verify the DataFrame\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Analyze class distribution for ethnicity\n",
    "print(\"\\nEthnicity Distribution Before Balancing:\")\n",
    "print(df['ethnicity'].value_counts())\n",
    "\n",
    "# Extract labels\n",
    "age_labels = df['age'].values\n",
    "gender_labels = df['gender'].values\n",
    "ethnicity_labels = df['ethnicity'].values\n",
    "\n",
    "# One-Hot Encode Gender and Ethnicity Labels\n",
    "gender_labels_cat = to_categorical(gender_labels, num_classes=2)\n",
    "ethnicity_labels_cat = to_categorical(ethnicity_labels, num_classes=5)\n",
    "\n",
    "# Create a balanced dataset for ethnicity by oversampling minority classes\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df['ethnicity'] == 0]\n",
    "df_minority_1 = df[df['ethnicity'] == 1]\n",
    "df_minority_2 = df[df['ethnicity'] == 2]\n",
    "df_minority_3 = df[df['ethnicity'] == 3]\n",
    "df_minority_4 = df[df['ethnicity'] == 4]\n",
    "\n",
    "# Find the maximum class count\n",
    "max_count = df['ethnicity'].value_counts().max()\n",
    "\n",
    "# Oversample minority classes\n",
    "df_minority_1_upsampled = resample(\n",
    "    df_minority_1,\n",
    "    replace=True,\n",
    "    n_samples=max_count,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_minority_2_upsampled = resample(\n",
    "    df_minority_2,\n",
    "    replace=True,\n",
    "    n_samples=max_count,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_minority_3_upsampled = resample(\n",
    "    df_minority_3,\n",
    "    replace=True,\n",
    "    n_samples=max_count,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_minority_4_upsampled = resample(\n",
    "    df_minority_4,\n",
    "    replace=True,\n",
    "    n_samples=max_count,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine majority class with upsampled minority classes\n",
    "df_balanced = pd.concat([\n",
    "    df_majority,\n",
    "    df_minority_1_upsampled,\n",
    "    df_minority_2_upsampled,\n",
    "    df_minority_3_upsampled,\n",
    "    df_minority_4_upsampled\n",
    "])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verify the new class distribution\n",
    "print(\"\\nEthnicity Distribution After Balancing:\")\n",
    "print(df_balanced['ethnicity'].value_counts())\n",
    "\n",
    "# Update labels after balancing\n",
    "age_labels_balanced = df_balanced['age'].values\n",
    "gender_labels_balanced = df_balanced['gender'].values\n",
    "ethnicity_labels_balanced = df_balanced['ethnicity'].values\n",
    "\n",
    "# One-Hot Encode Gender and Ethnicity Labels\n",
    "gender_labels_balanced_cat = to_categorical(gender_labels_balanced, num_classes=2)\n",
    "ethnicity_labels_balanced_cat = to_categorical(ethnicity_labels_balanced, num_classes=5)\n",
    "\n",
    "# Extract filenames\n",
    "filenames_balanced = df_balanced['img_name'].values\n",
    "\n",
    "# Split the data into training+validation and test sets\n",
    "\n",
    "# First, split into training+validation and test sets\n",
    "filenames_temp, filenames_test, age_temp, age_test, gender_temp, gender_test, ethnicity_temp, ethnicity_test = train_test_split(\n",
    "    filenames_balanced, age_labels_balanced, gender_labels_balanced_cat, ethnicity_labels_balanced_cat, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Then, split training+validation set into training and validation sets\n",
    "filenames_train, filenames_val, age_train, age_val, gender_train, gender_val, ethnicity_train, ethnicity_val = train_test_split(\n",
    "    filenames_temp, age_temp, gender_temp, ethnicity_temp, test_size=0.1765, random_state=42\n",
    ")\n",
    "\n",
    "# Verify the shapes after splitting\n",
    "print('\\nNumber of training samples:', len(filenames_train))\n",
    "print('Number of validation samples:', len(filenames_val))\n",
    "print('Number of test samples:', len(filenames_test))\n",
    "\n",
    "# Prepare labels as dictionaries\n",
    "train_labels = {\n",
    "    'age_output': age_train,\n",
    "    'gender_output': gender_train,\n",
    "    'ethnicity_output': ethnicity_train\n",
    "}\n",
    "\n",
    "val_labels = {\n",
    "    'age_output': age_val,\n",
    "    'gender_output': gender_val,\n",
    "    'ethnicity_output': ethnicity_val\n",
    "}\n",
    "\n",
    "test_labels = {\n",
    "    'age_output': age_test,\n",
    "    'gender_output': gender_test,\n",
    "    'ethnicity_output': ethnicity_test\n",
    "}\n",
    "\n",
    "# Create custom data generator\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_filenames, labels, batch_size, img_dir, img_height, img_width,\n",
    "                 shuffle=True, augment=False, return_sample_weights=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_filenames = image_filenames\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_dir = img_dir\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.return_sample_weights = return_sample_weights\n",
    "        self.indexes = np.arange(len(self.image_filenames))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        # Define augmentation parameters if augment is True\n",
    "        if self.augment:\n",
    "            self.datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                rotation_range=10,\n",
    "                width_shift_range=0.1,\n",
    "                height_shift_range=0.1,\n",
    "                horizontal_flip=True\n",
    "            )\n",
    "        else:\n",
    "            self.datagen = None\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_filenames) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate indexes of the batch\n",
    "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        batch_filenames = [self.image_filenames[k] for k in batch_indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y, sample_weights = self.__data_generation(batch_filenames, batch_indexes)\n",
    "        \n",
    "        if self.return_sample_weights:\n",
    "            return X, y, sample_weights  # Return sample weights\n",
    "        else:\n",
    "            return X, y\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, batch_filenames, batch_indexes):\n",
    "        X = []\n",
    "        y = {\n",
    "            'age_output': self.labels['age_output'][batch_indexes],\n",
    "            'gender_output': self.labels['gender_output'][batch_indexes],\n",
    "            'ethnicity_output': self.labels['ethnicity_output'][batch_indexes]\n",
    "        }\n",
    "        \n",
    "        # Initialize sample weights\n",
    "        sample_weights = {\n",
    "            'age_output': np.ones(len(batch_indexes)),\n",
    "            'gender_output': np.ones(len(batch_indexes)),\n",
    "            'ethnicity_output': np.ones(len(batch_indexes))\n",
    "        }\n",
    "        \n",
    "        # Compute sample weights for ethnicity_output\n",
    "        for i in range(len(batch_indexes)):\n",
    "            # Get the class index for ethnicity (one-hot to class index)\n",
    "            ethnicity_class_idx = np.argmax(y['ethnicity_output'][i])\n",
    "            sample_weights['ethnicity_output'][i] = ethnicity_class_weights[ethnicity_class_idx]\n",
    "        \n",
    "        # Load and preprocess images\n",
    "        for i, filename in enumerate(batch_filenames):\n",
    "            img_path = os.path.join(self.img_dir, filename)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img = img.convert('RGB')\n",
    "                img = img.resize((self.img_width, self.img_height))\n",
    "                img_array = img_to_array(img)\n",
    "                img_array = img_array / 255.0\n",
    "                \n",
    "                # Apply augmentation if enabled\n",
    "                if self.augment and self.datagen is not None:\n",
    "                    img_array = self.datagen.random_transform(img_array)\n",
    "                \n",
    "                X.append(img_array)\n",
    "            except Exception as e:\n",
    "                print(f'Error processing image {img_path}: {e}')\n",
    "                X.append(np.zeros((self.img_height, self.img_width, 3)))\n",
    "        \n",
    "        X = np.array(X)\n",
    "        return X, y, sample_weights\n",
    "\n",
    "\n",
    "# Create generators\n",
    "train_generator = DataGenerator(\n",
    "    filenames_train,\n",
    "    train_labels,\n",
    "    batch_size=batch_size,\n",
    "    img_dir=dataset_dir,\n",
    "    img_height=IMG_HEIGHT,\n",
    "    img_width=IMG_WIDTH,\n",
    "    augment=True,  # Enable augmentation for training data\n",
    "    return_sample_weights=True  # Return sample weights\n",
    ")\n",
    "\n",
    "\n",
    "val_generator = DataGenerator(\n",
    "    filenames_val,\n",
    "    val_labels,\n",
    "    batch_size=batch_size,\n",
    "    img_dir=dataset_dir,\n",
    "    img_height=IMG_HEIGHT,\n",
    "    img_width=IMG_WIDTH\n",
    "    # return_sample_weights=False (default)\n",
    ")\n",
    "\n",
    "test_generator = DataGenerator(\n",
    "    filenames_test,\n",
    "    test_labels,\n",
    "    batch_size=batch_size,\n",
    "    img_dir=dataset_dir,\n",
    "    img_height=IMG_HEIGHT,\n",
    "    img_width=IMG_WIDTH,\n",
    "    shuffle=False  # No need to shuffle test data\n",
    "    # return_sample_weights=False (default)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Build the multi-task learning model\n",
    "input_layer = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# Shared Convolutional Layers\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output Layers with Correct Names in Dense layers\n",
    "age_output = Dense(1, name='age_output')(x)\n",
    "gender_output = Dense(2, activation='softmax', name='gender_output')(x)\n",
    "ethnicity_output = Dense(5, activation='softmax', name='ethnicity_output')(x)\n",
    "\n",
    "# Create the Model\n",
    "outputs = {\n",
    "    'age_output': age_output,\n",
    "    'gender_output': gender_output,\n",
    "    'ethnicity_output': ethnicity_output\n",
    "}\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "\n",
    "# Verify the model's output names\n",
    "print('\\nModel output names:', model.output_names)\n",
    "\n",
    "# Compute class weights for ethnicity\n",
    "import numpy as np\n",
    "ethnicity_classes = np.unique(df_balanced['ethnicity'])\n",
    "ethnicity_class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=ethnicity_classes,\n",
    "    y=df_balanced['ethnicity']\n",
    ")\n",
    "ethnicity_class_weights = dict(enumerate(ethnicity_class_weights))\n",
    "\n",
    "print('\\nEthnicity class weights:', ethnicity_class_weights)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'age_output': 'mse',\n",
    "        'gender_output': 'categorical_crossentropy',\n",
    "        'ethnicity_output': 'categorical_crossentropy'\n",
    "    },\n",
    "    metrics={\n",
    "        'age_output': 'mae',\n",
    "        'gender_output': 'accuracy',\n",
    "        'ethnicity_output': 'accuracy'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set up callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_multitask_model.keras',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss'\n",
    ")\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# Train the model using generators\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    "    # Removed class_weight parameter\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate(test_generator, verbose=2)\n",
    "\n",
    "# Print the evaluation results\n",
    "print('\\nTest loss and metrics:', test_loss)\n",
    "\n",
    "# Save the model\n",
    "model.save('multitask_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
