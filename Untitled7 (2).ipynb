{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcda7a05-3a44-4268-a970-1efdc3e0fafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of one-hot encoded ethnicities: [[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │ input_layer_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ flatten_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vgg16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ age_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,089</span> │ flatten_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gender_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,089</span> │ flatten_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ ethnicity_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">125,445</span> │ flatten_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_27 (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │      \u001b[38;5;34m14,714,688\u001b[0m │ input_layer_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ flatten_13 (\u001b[38;5;33mFlatten\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │ vgg16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ age_output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │          \u001b[38;5;34m25,089\u001b[0m │ flatten_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gender_output (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │          \u001b[38;5;34m25,089\u001b[0m │ flatten_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ ethnicity_output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                 │         \u001b[38;5;34m125,445\u001b[0m │ flatten_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,890,311</span> (56.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,890,311\u001b[0m (56.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">175,623</span> (686.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m175,623\u001b[0m (686.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images shape: (16, 224, 224, 3)\n",
      "Batch ages shape: (16, 1)\n",
      "Batch genders shape: (16, 1)\n",
      "Batch ethnicities shape: (16, 5)\n",
      "Sample train images shape: (16, 224, 224, 3)\n",
      "Sample train labels shapes: (16, 1)\n",
      "Sample train labels shapes: (16, 1)\n",
      "Sample train labels shapes: (16, 5)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 5), output.shape=(None, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 196\u001b[0m\n\u001b[0;32m    193\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m validation_dataset\u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    202\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\face_recognition\\env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\face_recognition\\env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:587\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n\u001b[1;32m--> 587\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    588\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    591\u001b[0m         )\n\u001b[0;32m    593\u001b[0m output, from_logits \u001b[38;5;241m=\u001b[39m _get_logits(\n\u001b[0;32m    594\u001b[0m     output, from_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    595\u001b[0m )\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 5), output.shape=(None, 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = 'UTKface'\n",
    "\n",
    "# Initialize lists to hold data\n",
    "file_names = []\n",
    "ages = []\n",
    "genders = []\n",
    "ethnicities = []\n",
    "\n",
    "# Define the target image size\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# Iterate through the dataset and process each image\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        # Extract age, gender, and ethnicity from the filename\n",
    "        try:\n",
    "            age, gender, ethnicity, _ = filename.split('_')\n",
    "            age = int(age)\n",
    "            gender = int(gender)\n",
    "            ethnicity = int(ethnicity)\n",
    "\n",
    "            # Append data to lists\n",
    "            file_names.append(filename)\n",
    "            ages.append(age)\n",
    "            genders.append(gender)\n",
    "            ethnicities.append(ethnicity)\n",
    "\n",
    "        except ValueError:\n",
    "            # Skip files that don't follow the expected naming convention\n",
    "            continue\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "ages = np.array(ages, dtype=np.float32).reshape(-1, 1) / 116.0  # Normalize age values (max age is 116)\n",
    "genders = np.array(genders, dtype=np.float32).reshape(-1, 1)  # Reshape to (n_samples, 1)\n",
    "ethnicities = to_categorical(ethnicities, num_classes=5)  # One-hot encode ethnicities\n",
    "print(\"Sample of one-hot encoded ethnicities:\", ethnicities[:5])\n",
    "\n",
    "\n",
    "# Train and validation split\n",
    "split_idx = int(0.8 * len(file_names))\n",
    "train_file_names, val_file_names = file_names[:split_idx], file_names[split_idx:]\n",
    "train_ages, val_ages = ages[:split_idx], ages[split_idx:]\n",
    "train_genders, val_genders = genders[:split_idx], genders[split_idx:]\n",
    "train_ethnicities, val_ethnicities = ethnicities[:split_idx], ethnicities[split_idx:]\n",
    "\n",
    "# Custom Data Generator for multi-output (age, gender, ethnicity)\n",
    "class MultiOutputDataGenerator(Sequence):\n",
    "    def __init__(self, file_names, ages, genders, ethnicities, batch_size, img_size, dataset_path):\n",
    "        self.file_names = file_names\n",
    "        self.ages = ages\n",
    "        self.genders = genders\n",
    "        self.ethnicities = ethnicities\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.file_names) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_files = self.file_names[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_ages = self.ages[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_genders = self.genders[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_ethnicities = self.ethnicities[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "    \n",
    "        # Initialize batch arrays\n",
    "        batch_images = np.zeros((len(batch_files), *self.img_size, 3), dtype=np.float32)\n",
    "    \n",
    "        for i, file_name in enumerate(batch_files):\n",
    "            img = Image.open(os.path.join(self.dataset_path, file_name)).convert('RGB')\n",
    "            img = img.resize(self.img_size)\n",
    "            img_array = np.array(img) / 255.0  # Normalize\n",
    "            batch_images[i] = img_array\n",
    "    \n",
    "        print(f\"Batch images shape: {batch_images.shape}\")\n",
    "        print(f\"Batch ages shape: {batch_ages.shape}\")\n",
    "        print(f\"Batch genders shape: {batch_genders.shape}\")\n",
    "        print(f\"Batch ethnicities shape: {batch_ethnicities.shape}\")\n",
    "    \n",
    "        # Return images and corresponding outputs as a tuple\n",
    "        return batch_images, {\n",
    "            'age_output': np.array(batch_ages, dtype=np.float32), \n",
    "            'gender_output': np.array(batch_genders, dtype=np.float32), \n",
    "            'ethnicity_output': np.array(batch_ethnicities, dtype=np.float32)\n",
    "        }\n",
    "\n",
    "\n",
    "# Create train and validation generators\n",
    "batch_size = 16\n",
    "\n",
    "train_generator = MultiOutputDataGenerator(\n",
    "    train_file_names, train_ages, train_genders, train_ethnicities, batch_size, IMG_SIZE, dataset_path\n",
    ")\n",
    "\n",
    "validation_generator = MultiOutputDataGenerator(\n",
    "    val_file_names, val_ages, val_genders, val_ethnicities, batch_size, IMG_SIZE, dataset_path\n",
    ")\n",
    "\n",
    "# Load the VGG16 model without the top layer\n",
    "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "# Freeze the VGG16 layers\n",
    "for layer in vgg16_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Define the multi-output model\n",
    "inputs = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "x = vgg16_base(inputs)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Age output (regression)\n",
    "age_output = Dense(1, activation='linear', name='age_output')(x)\n",
    "\n",
    "# Gender output (binary classification)\n",
    "gender_output = Dense(1, activation='sigmoid', name='gender_output')(x)\n",
    "\n",
    "# Ethnicity output (multi-class classification)\n",
    "ethnicity_output = Dense(5, activation='softmax', name='ethnicity_output')(x)\n",
    "\n",
    "# Define the complete model\n",
    "model = Model(inputs=inputs, outputs=[age_output, gender_output, ethnicity_output])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'age_output': 'mean_squared_error',\n",
    "        'gender_output': 'binary_crossentropy',\n",
    "        'ethnicity_output': 'categorical_crossentropy'\n",
    "    },\n",
    "    metrics={\n",
    "        'age_output': 'mae',\n",
    "        'gender_output': 'accuracy',\n",
    "        'ethnicity_output': 'accuracy'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Set up a callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Train the model using the generators\n",
    "EPOCHS = 20\n",
    "\n",
    "# Print sample outputs from the generator to check shapes and types\n",
    "sample_train_images, sample_train_labels = next(iter(train_generator))\n",
    "print(f\"Sample train images shape: {sample_train_images.shape}\")\n",
    "print(f\"Sample train labels shapes: {sample_train_labels['age_output'].shape}\")\n",
    "print(f\"Sample train labels shapes: {sample_train_labels['gender_output'].shape}\")\n",
    "print(f\"Sample train labels shapes: {sample_train_labels['ethnicity_output'].shape}\")\n",
    "\n",
    "# Convert train and validation generators to tf.data.Dataset\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, IMG_SIZE[0], IMG_SIZE[1], 3), dtype=tf.float32),\n",
    "        {\n",
    "            'age_output': tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "            'gender_output': tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "            'ethnicity_output': tf.TensorSpec(shape=(None, 5), dtype=tf.float32)\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: validation_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, IMG_SIZE[0], IMG_SIZE[1], 3), dtype=tf.float32),\n",
    "        {\n",
    "            'age_output': tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "            'gender_output': tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "            'ethnicity_output': tf.TensorSpec(shape=(None, 5), dtype=tf.float32)\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prefetch data for faster consumption\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5944b44b-2ca7-4cea-8c05-76d3989e5390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images shape: (16, 224, 224, 3)\n",
      "Batch ages shape: (16, 1)\n",
      "Batch genders shape: (16, 1)\n",
      "Batch ethnicities shape: (16, 5)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Predicted ages shape: (16, 1)\n",
      "Predicted genders shape: (16, 1)\n",
      "Predicted ethnicities shape: (16, 5)\n"
     ]
    }
   ],
   "source": [
    "# Test the generator and model output before training\n",
    "batch_images, batch_labels = train_generator[0]\n",
    "predictions = model.predict(batch_images)\n",
    "\n",
    "print(f\"Predicted ages shape: {predictions[0].shape}\")\n",
    "print(f\"Predicted genders shape: {predictions[1].shape}\")\n",
    "print(f\"Predicted ethnicities shape: {predictions[2].shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
